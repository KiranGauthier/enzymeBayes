{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simple Brownian Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional distribution $p(x_k\\mid x_{k-1})$ depends on the diffusion coefficient $D$, the displacement $\\Delta x_k$ as well as the lag time $\\Delta t_k$ via the following expression: \n",
    "\n",
    "$$p(x_k\\mid x_{k-1}) = \\frac{1}{\\sqrt{4\\pi D\\tau_k}}exp\\left(-\\frac{\\Delta (x_k-x_{k-1})^2}{4D\\tau_k}\\right)$$\n",
    "\n",
    "and the joint distribution in terms of the displacement in 2D is: \n",
    "\n",
    "$$p(dx, dy \\mid D) = \\prod_{k=1}^M\\left(\\frac{1}{4\\pi D \\tau_k}\\right)exp\\left(-\\sum_{k=1}^M\\frac{dx^2_k + dy^2_k}{4D \\tau_k}\\right)$$\n",
    "\n",
    "D has an inverse gamma prior: \n",
    "\n",
    "$$p(D \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\text{Gamma}(\\alpha)}D^{-\\alpha-1}exp\\left(-\\frac{\\beta}{D}\\right) $$\n",
    "\n",
    "The marginal likelihood is : \n",
    "\n",
    "$$p(dx, dy) = \\int p(dx, dy, D)dD = \\frac{\\beta^{\\alpha}\\text{Gamma}(\\alpha_0)}{\\beta_0^{\\alpha_0}\\text{Gamma}(\\alpha)}\\prod_{k=1}^M\\left(\\frac{1}{4\\pi\\tau_k}\\right) $$\n",
    "\n",
    "where, \n",
    "\n",
    "$$\\alpha_0 = M+\\alpha$$\n",
    "$$\\beta_0 = \\sum_{k=1}^M\\frac{dx^2_k + dy^2_k}{4\\tau_k}+\\beta $$\n",
    "\n",
    "The analytical full posterior is then: \n",
    "\n",
    "$$p(D \\mid dx, dy) = \\frac{\\beta_0^{\\alpha_0}}{\\text{Gamma}(\\alpha_0)}D^{-\\alpha_0 - 1}exp\\left(-\\frac{\\beta_0}{D}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Motion with Measurement Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of experimental data $X_k$ conditioned on true data $x_k$ with measurement noise $\\sigma$ can be epxressed as: \n",
    "\n",
    "$$ p(X_k\\mid x_k, \\sigma) = \\frac{1}{2\\pi \\sigma^2} \\exp\\left(-\\frac{(X_k-x_k)^2}{2 \\sigma^2}\\right)$$\n",
    "\n",
    "The liklihood of $X_k$ can be obtained by marginalizing over $x_k$: \n",
    "\n",
    "$$ p(\\mathbf{X}\\mid \\sigma) = \\int d\\mathbf{x}~p(\\mathbf{X}\\mid \\mathbf{x},\\sigma)p(\\mathbf{x}\\mid \\sigma)$$\n",
    "\n",
    "where $p(x\\mid \\sigma)$ is the multivarite normal distribution obtained by transforming the univariate Brownian likelihood with mean $\\mu$ and covarience matrix $\\Sigma$. Based on the theorem provided in Koller' graph model textbook p 251, the above integral can be evaluted to yield the likilhood $p(\\mathbf{X}\\mid \\sigma)$ with mean $\\mu$ and covarience matrix $\\Sigma + I\\sigma^2$. $\\Sigma^-1$ is: \n",
    "\n",
    "$$ \\Sigma^{-1}_{k,k} = \\frac{1}{2D\\Delta t_k} + \\frac{1}{2D\\Delta t_{k+1}} \\quad \\Sigma^{-1}_{k,k-1} = -\\frac{1}{2D\\Delta t_{k}} \\quad \\Sigma^{-1}_{k,k+1} = -\\frac{1}{2D\\Delta t_{k+1}} $$\n",
    "\n",
    "$$ \\Sigma^{-1}_{0,0} = \\frac{1}{\\sigma_0^2} + \\frac{1}{2D\\Delta t_1} \\quad \\Sigma^{-1}_{0,1} = -\\frac{1}{2D\\Delta t_1} $$\n",
    "\n",
    "$$ \\Sigma^{-1}_{N,N} = \\frac{1}{2D\\Delta t_N} \\quad \\Sigma^{-1}_{N,N-1} = -\\frac{1}{2D\\Delta t_N} $$\n",
    "\n",
    "It turns out that, we can find the cov matrix using mathmetica, so that it will be less computational demanding during inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the confined enzyme motion, we are going to say that those enzyme are actually stuck on the hydrophobic glass surface, and the likelihood function for each observation is simply: \n",
    "\n",
    "$$ p(X_k\\mid \\sigma) = \\int p(X_0) \\prod_{k=0}^N \\frac{1}{2\\pi \\sigma^2} \\exp\\left(-\\frac{(X_k-X_0)^2}{2 \\sigma^2}\\right)dX_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Harmonic Potential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mode, the enzyme position ${x}(t)$ is governed by the overdamped Langevin equation for diffusion in a harmonic potential well: \n",
    "\n",
    "$$    \\dot{{x}} = \\frac{k}{\\gamma} ({x}-{x}_{c}) + {\\xi}(t)$$\n",
    "\n",
    "Here, $k$ is a spring constant characterizing the strength of the confining potential centered at ${x}_{c}$, $\\gamma$ is a friction coefficient, and ${\\xi}(t)$ is white Gaussian noise with covariance $\\langle {x}(t){x}(t')\\rangle = 2 k_B T/\\gamma \\, {\\delta}(t-t')$.  Alternatively, this problem can be parameterized in terms of the diffusion coefficient $D=k_B T/\\gamma$ and the rate parameter $\\lambda = k/\\gamma$.  As the $x$ and $y$ components of particle motion evolve independently, we focus first on the $x$ component.  The Langevin equation describes an Ornsteinâ€“Uhlenbeck process, in which the transition probability from position $x_{k-1}$ at time $t_{k-1}$ to position $x_k$ at time $t_k$ is normally distributed as: \n",
    "\n",
    "$$    x_k\\mid x_{k-1}, D,\\lambda,x_{c} \\sim \\mathcal{N}\\left(x_{c} + (x_{k-1}-x_{c}) e^{-\\lambda\\tau_{k}},~\\frac{D}{\\lambda} ( 1- e^{-2 \\lambda \\tau_{k}} )\\right) $$\n",
    "\n",
    "We can apply the same transformation as we did for the BM model and turn the likelihood into a multivariate guassian distribution with cov matrix as: \n",
    "\n",
    "$$(\\Sigma_x)^{-1}_{kk} = \\frac{\\lambda}{D} \\left(\\frac{1}{1-e^{-2\\lambda  \\tau_k}} + \\frac{e^{-2\\lambda  \\tau_{k+1}}}{1-e^{-2 \\lambda  \\tau_{k+1}}}\\right)$$\n",
    "\n",
    "$$\n",
    "(\\Sigma_x)^{-1}_{k,k-1} = -\\frac{\\lambda}{D} \\left(\\frac{e^{-2\\lambda  \\tau_{k}}}{1-e^{-2\\lambda  \\tau_k}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\Sigma_x)^{-1}_{k,k+1} = -\\frac{\\lambda}{D} \\left(\\frac{e^{-2\\lambda  \\tau_{k+1}}}{1-e^{-2\\lambda  \\tau_{k+1}}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical HPW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to assume that the each enzyme will have an unique diffusion coefficient and experiences a different well strength. However, we are also going to say that all the diffusion coefficient are drawn from a common population Lognormal distribution, charaterized by population level mean and standard deviation. The way we used to recover population level paramters are a bit unusual: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginalized likehood is: \n",
    "\n",
    "$$ p(\\Delta \\mathbf{X}\\mid\\mu_p, \\sigma_p) = \\prod_i \\int d\\text{ln}(D_i) p(\\Delta \\mathbf{x}_i\\mid \\text{ln}(D_i)) p(\\text{ln}(D_i) \\mid \\mu_p, \\sigma_p)$$\n",
    "\n",
    "where the two relevent distributions are: \n",
    "\n",
    "$$p(\\Delta \\mathbf{x}_i\\mid \\text{ln}(D_i))=\\prod_{j}\\frac{1}{\\sqrt{4\\pi e^{\\text{ln}(D_i)} \\Delta t}}exp(-\\frac{\\Delta \\mathbf{x}_{i,j}^2}{4e^{\\text{ln}(D_i)}\\Delta t}) \\quad p(\\text{ln}(D_i)\\mid \\mu_{p},\\sigma_{p})=\\frac{1}{\\sqrt{2\\pi\\sigma_{p}^2}}exp(-\\frac{(\\text{ln}(D_i)-\\mu_p)^2}{2\\sigma_{p}^2})$$\n",
    "\n",
    "The Integral can be obtained in a closed form if we use normal approximation of the likelihhod: \n",
    "\n",
    "$$p(\\Delta \\mathbf{x}_i\\mid \\text{ln}(D_i)) = N(\\text{ln}(D_i) \\mid \\mu_i, \\sigma_i) $$\n",
    "\n",
    "$\\mu_i$ and $\\sigma_i$ are usually obtained via map or calculating different moments, but in this case we can use the sampled posterior for diffusion coeffcient.\n",
    "\n",
    "The final likelihood, by solving the integral is then: \n",
    "\n",
    "$$p(\\mu_i, \\sigma_i\\mid\\mu_p, \\sigma_p) = \\prod_i \\frac{1}{ \\sqrt{2\\pi\\sigma_{i}^2+ \\sigma_{p}^2}}exp(-\\frac{(\\mu_i-\\mu_p)^2}{2(\\sigma_{i}^2+ \\sigma_{p}^2)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, and all below cases, we used the [No U-Turn Hamiltonian Monte Carlo](http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf) sampler implemented in [PyMC3](https://docs.pymc.io/) for Python as it explored the low dimensional parameter set efficiently, all of which were continuous parameters. This is due to the fact that HMC+NUTS was able to leverage the gradient of the log joint density and converge much faster than other sampling methods, while also automatically tuning its sampling parameters. PyMC3 is built on Theano to analytically compute these model gradients through automatic differentiation of the log joint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
